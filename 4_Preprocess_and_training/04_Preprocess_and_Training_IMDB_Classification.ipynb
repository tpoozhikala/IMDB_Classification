{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. IMDB_Classification_Preprocess_and_Training<a id='2_Data_wrangling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Table of Contents<a id='2.1_Contents'></a>\n",
    "* 4. Bank_Churnrate_Preprocess_and_Training\n",
    "  * 4.1 Table of Contents\n",
    "  * 4.2 Introduction\n",
    "  * 4.3 Imports\n",
    "  * 4.4 Load Bank Churn Missing and Dropped Datasets\n",
    "    * 4.4.1 Intial Loading and Assessments of Datasets\n",
    "    * 4.4.2 Avoid Multicollineraty (Dummy Variable Trap)\n",
    "  * 4.5 Create Train_Test Split & Preprocess Training Data\n",
    "    * 4.5.1 Create Train_Test_Split\n",
    "    * 4.5.2 Apply StandardScaler() for Training Data\n",
    "  * 4.6 Save Data\n",
    "  * 4.7 Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that a thorough analysis as well as further cleaning and One-Hot enconding steps were implemented in prior EDA, train and test sets based on a usual 70/30 split will be created based on both the missing and dropped bank churn datasets for future steps in assessing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Imports<a id='2.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all appropriate packages in order to develop associated train and test sets for both imputed and dropped datasets for bank churn rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pandas, matplotlib.pyplot, seaborn, and associated scikit learn methods and functions as well as random number for reproducibility\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "random_number = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Load Bank Churn Missing and Dropped Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Intial Loading and Assessments of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading associated datasets of dropped vs. missing datasets for movie_df_filtered.\n",
    "path_file = 'C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/IMDB_Classification/0_Datasets/movie_df_filtered.csv'\n",
    "movie_df_filtered = pd.read_csv(path_file, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auditing the datasets with .info() and .head() displaying the first few records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3939 entries, 0 to 3971\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       3939 non-null   object\n",
      " 1   image    3939 non-null   object\n",
      " 2   title    3939 non-null   object\n",
      " 3   Genre_1  3939 non-null   object\n",
      " 4   Genre_2  3939 non-null   object\n",
      " 5   Genre_3  3939 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 215.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#.info() on bank_missing_df and bank_dropped_df to see a summary of the data\n",
    "movie_df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>title</th>\n",
       "      <th>Genre_1</th>\n",
       "      <th>Genre_2</th>\n",
       "      <th>Genre_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0099785</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMzFkM2...</td>\n",
       "      <td>Home Alone</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Family</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0100944</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMjI1MD...</td>\n",
       "      <td>The Witches</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0099810</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BZDdkOD...</td>\n",
       "      <td>The Hunt for Red October</td>\n",
       "      <td>Action</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0099088</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BYjhlMG...</td>\n",
       "      <td>Back to the Future Part III</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0100419</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BYzk3NG...</td>\n",
       "      <td>Problem Child</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Family</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              image  \\\n",
       "0  tt0099785  https://m.media-amazon.com/images/M/MV5BMzFkM2...   \n",
       "1  tt0100944  https://m.media-amazon.com/images/M/MV5BMjI1MD...   \n",
       "2  tt0099810  https://m.media-amazon.com/images/M/MV5BZDdkOD...   \n",
       "3  tt0099088  https://m.media-amazon.com/images/M/MV5BYjhlMG...   \n",
       "4  tt0100419  https://m.media-amazon.com/images/M/MV5BYzk3NG...   \n",
       "\n",
       "                         title    Genre_1     Genre_2    Genre_3  \n",
       "0                   Home Alone     Comedy      Family       None  \n",
       "1                  The Witches  Adventure      Comedy     Family  \n",
       "2     The Hunt for Red October     Action   Adventure   Thriller  \n",
       "3  Back to the Future Part III  Adventure      Comedy     Sci-Fi  \n",
       "4                Problem Child     Comedy      Family       None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#head method on bank_missing_df and bank_dropped_df to print the first several rows of the data\n",
    "movie_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3939, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Avoid Multicollineraty (Dummy Variable Trap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid the dummy variable trap or potentially having issues with the feautres because of multicollineraty within the dummy variables, the different one-hot enconded categorical varibles will have k-1 features (where k is the number of unique categories within the variable). The baseline values will be values of the categories that occur most frequently in the respective categorical variable (ex. Card_Category_Blue). \n",
    "\n",
    "Further information and reference for the above paragraph found on: https://www.statology.org/dummy-variable-trap/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Genre_1_Action</th>\n",
       "      <th>Genre_1_Adventure</th>\n",
       "      <th>Genre_1_Animation</th>\n",
       "      <th>Genre_1_Biography</th>\n",
       "      <th>Genre_1_Comedy</th>\n",
       "      <th>Genre_1_Crime</th>\n",
       "      <th>Genre_1_Documentary</th>\n",
       "      <th>Genre_1_Drama</th>\n",
       "      <th>Genre_1_Family</th>\n",
       "      <th>...</th>\n",
       "      <th>Genre_3_ Musical</th>\n",
       "      <th>Genre_3_ Mystery</th>\n",
       "      <th>Genre_3_ News</th>\n",
       "      <th>Genre_3_ Romance</th>\n",
       "      <th>Genre_3_ Sci-Fi</th>\n",
       "      <th>Genre_3_ Sport</th>\n",
       "      <th>Genre_3_ Thriller</th>\n",
       "      <th>Genre_3_ War</th>\n",
       "      <th>Genre_3_ Western</th>\n",
       "      <th>Genre_3_None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0099785</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0100944</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0099810</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0099088</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0100419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  Genre_1_Action  Genre_1_Adventure  Genre_1_Animation  \\\n",
       "0  tt0099785               0                  0                  0   \n",
       "1  tt0100944               0                  1                  0   \n",
       "2  tt0099810               1                  0                  0   \n",
       "3  tt0099088               0                  1                  0   \n",
       "4  tt0100419               0                  0                  0   \n",
       "\n",
       "   Genre_1_Biography  Genre_1_Comedy  Genre_1_Crime  Genre_1_Documentary  \\\n",
       "0                  0               1              0                    0   \n",
       "1                  0               0              0                    0   \n",
       "2                  0               0              0                    0   \n",
       "3                  0               0              0                    0   \n",
       "4                  0               1              0                    0   \n",
       "\n",
       "   Genre_1_Drama  Genre_1_Family  ...  Genre_3_ Musical  Genre_3_ Mystery  \\\n",
       "0              0               0  ...                 0                 0   \n",
       "1              0               0  ...                 0                 0   \n",
       "2              0               0  ...                 0                 0   \n",
       "3              0               0  ...                 0                 0   \n",
       "4              0               0  ...                 0                 0   \n",
       "\n",
       "   Genre_3_ News  Genre_3_ Romance  Genre_3_ Sci-Fi  Genre_3_ Sport  \\\n",
       "0              0                 0                0               0   \n",
       "1              0                 0                0               0   \n",
       "2              0                 0                0               0   \n",
       "3              0                 0                1               0   \n",
       "4              0                 0                0               0   \n",
       "\n",
       "   Genre_3_ Thriller  Genre_3_ War  Genre_3_ Western  Genre_3_None  \n",
       "0                  0             0                 0             1  \n",
       "1                  0             0                 0             0  \n",
       "2                  1             0                 0             0  \n",
       "3                  0             0                 0             0  \n",
       "4                  0             0                 0             1  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_dummy = movie_df_filtered[['id','Genre_1', 'Genre_2', 'Genre_3']]\n",
    "movie_dummy = pd.get_dummies(movie_dummy, columns = ['Genre_1','Genre_2','Genre_3'])\n",
    "movie_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (1000,1000)\n",
    "img = Image.open('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/IMDB_Classification/0_Datasets/Image_1/Image_1/tt3205010.jpg')\n",
    "img_re = img.resize(shape)\n",
    "img_re.show()\n",
    "img_array = np.asarray(img_re)\n",
    "img_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_2 = Image.open('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/IMDB_Classification/0_Datasets/Image_1/Image_1/tt3210796.jpg')\n",
    "img2_re = img_2.resize(shape)\n",
    "img2_re.show()\n",
    "img_2array = np.asarray(img2_re)\n",
    "img_2array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop features within both missing and dropped datasets to avoid dummy variable collineraty \n",
    "\n",
    "\n",
    "\n",
    "features_to_drop = ['Gender_F', 'Education_Level_Graduate', 'Marital_Status_Married', 'Income_Category_Less than $40K', 'Card_Category_Blue']\n",
    "\n",
    "bank_missing_df_dummy_dropped = bank_missing_df.drop(features_to_drop, axis=1)\n",
    "bank_dropped_df_dummy_dropped = bank_dropped_df.drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8101 entries, 0 to 8100\n",
      "Data columns (total 33 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Customer_Age                   8101 non-null   int64  \n",
      " 1   Dependent_count                8101 non-null   int64  \n",
      " 2   Months_on_book                 8101 non-null   int64  \n",
      " 3   Total_Relationship_Count       8101 non-null   int64  \n",
      " 4   Months_Inactive_12_mon         8101 non-null   int64  \n",
      " 5   Contacts_Count_12_mon          8101 non-null   int64  \n",
      " 6   Credit_Limit                   8101 non-null   float64\n",
      " 7   Total_Revolving_Bal            8101 non-null   int64  \n",
      " 8   Avg_Open_To_Buy                8101 non-null   float64\n",
      " 9   Total_Amt_Chng_Q4_Q1           8101 non-null   float64\n",
      " 10  Total_Trans_Amt                8101 non-null   int64  \n",
      " 11  Total_Trans_Ct                 8101 non-null   int64  \n",
      " 12  Total_Ct_Chng_Q4_Q1            8101 non-null   float64\n",
      " 13  Avg_Utilization_Ratio          8101 non-null   float64\n",
      " 14  Attrition_Flag                 8101 non-null   int64  \n",
      " 15  Gender_M                       8101 non-null   int64  \n",
      " 16  Education_Level_College        8101 non-null   int64  \n",
      " 17  Education_Level_Doctorate      8101 non-null   int64  \n",
      " 18  Education_Level_High School    8101 non-null   int64  \n",
      " 19  Education_Level_Post-Graduate  8101 non-null   int64  \n",
      " 20  Education_Level_Uneducated     8101 non-null   int64  \n",
      " 21  Education_Level_missing        8101 non-null   int64  \n",
      " 22  Marital_Status_Divorced        8101 non-null   int64  \n",
      " 23  Marital_Status_Single          8101 non-null   int64  \n",
      " 24  Marital_Status_missing         8101 non-null   int64  \n",
      " 25  Income_Category_$120K +        8101 non-null   int64  \n",
      " 26  Income_Category_$40K - $60K    8101 non-null   int64  \n",
      " 27  Income_Category_$60K - $80K    8101 non-null   int64  \n",
      " 28  Income_Category_$80K - $120K   8101 non-null   int64  \n",
      " 29  Income_Category_missing        8101 non-null   int64  \n",
      " 30  Card_Category_Gold             8101 non-null   int64  \n",
      " 31  Card_Category_Platinum         8101 non-null   int64  \n",
      " 32  Card_Category_Silver           8101 non-null   int64  \n",
      "dtypes: float64(5), int64(28)\n",
      "memory usage: 2.1 MB\n"
     ]
    }
   ],
   "source": [
    "#Assess new dataframe of dropped features bank_missing_df_dummy_dropped\n",
    "bank_missing_df_dummy_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Age</th>\n",
       "      <th>Dependent_count</th>\n",
       "      <th>Months_on_book</th>\n",
       "      <th>Total_Relationship_Count</th>\n",
       "      <th>Months_Inactive_12_mon</th>\n",
       "      <th>Contacts_Count_12_mon</th>\n",
       "      <th>Credit_Limit</th>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <th>Avg_Open_To_Buy</th>\n",
       "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
       "      <th>...</th>\n",
       "      <th>Marital_Status_Single</th>\n",
       "      <th>Marital_Status_missing</th>\n",
       "      <th>Income_Category_$120K +</th>\n",
       "      <th>Income_Category_$40K - $60K</th>\n",
       "      <th>Income_Category_$60K - $80K</th>\n",
       "      <th>Income_Category_$80K - $120K</th>\n",
       "      <th>Income_Category_missing</th>\n",
       "      <th>Card_Category_Gold</th>\n",
       "      <th>Card_Category_Platinum</th>\n",
       "      <th>Card_Category_Silver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3723.0</td>\n",
       "      <td>1728</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>0.595</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5396.0</td>\n",
       "      <td>1803</td>\n",
       "      <td>3593.0</td>\n",
       "      <td>0.493</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15987.0</td>\n",
       "      <td>1648</td>\n",
       "      <td>14339.0</td>\n",
       "      <td>0.732</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3625.0</td>\n",
       "      <td>2517</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>1.158</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>1926</td>\n",
       "      <td>794.0</td>\n",
       "      <td>0.602</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_Age  Dependent_count  Months_on_book  Total_Relationship_Count  \\\n",
       "0            54                1              36                         1   \n",
       "1            58                4              48                         1   \n",
       "2            45                4              36                         6   \n",
       "3            34                2              36                         4   \n",
       "4            49                2              39                         5   \n",
       "\n",
       "   Months_Inactive_12_mon  Contacts_Count_12_mon  Credit_Limit  \\\n",
       "0                       3                      3        3723.0   \n",
       "1                       4                      3        5396.0   \n",
       "2                       1                      3       15987.0   \n",
       "3                       3                      4        3625.0   \n",
       "4                       3                      4        2720.0   \n",
       "\n",
       "   Total_Revolving_Bal  Avg_Open_To_Buy  Total_Amt_Chng_Q4_Q1  ...  \\\n",
       "0                 1728           1995.0                 0.595  ...   \n",
       "1                 1803           3593.0                 0.493  ...   \n",
       "2                 1648          14339.0                 0.732  ...   \n",
       "3                 2517           1108.0                 1.158  ...   \n",
       "4                 1926            794.0                 0.602  ...   \n",
       "\n",
       "   Marital_Status_Single  Marital_Status_missing  Income_Category_$120K +  \\\n",
       "0                      1                       0                        0   \n",
       "1                      0                       0                        0   \n",
       "2                      1                       0                        0   \n",
       "3                      1                       0                        0   \n",
       "4                      0                       0                        0   \n",
       "\n",
       "   Income_Category_$40K - $60K  Income_Category_$60K - $80K  \\\n",
       "0                            0                            0   \n",
       "1                            0                            0   \n",
       "2                            0                            0   \n",
       "3                            0                            0   \n",
       "4                            1                            0   \n",
       "\n",
       "   Income_Category_$80K - $120K  Income_Category_missing  Card_Category_Gold  \\\n",
       "0                             0                        1                   0   \n",
       "1                             0                        1                   0   \n",
       "2                             0                        0                   1   \n",
       "3                             0                        0                   0   \n",
       "4                             0                        0                   0   \n",
       "\n",
       "   Card_Category_Platinum  Card_Category_Silver  \n",
       "0                       0                     0  \n",
       "1                       0                     0  \n",
       "2                       0                     0  \n",
       "3                       0                     0  \n",
       "4                       0                     0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_missing_df_dummy_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8101, 33)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_missing_df_dummy_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5690 entries, 3 to 8100\n",
      "Data columns (total 30 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Customer_Age                   5690 non-null   int64  \n",
      " 1   Dependent_count                5690 non-null   int64  \n",
      " 2   Months_on_book                 5690 non-null   int64  \n",
      " 3   Total_Relationship_Count       5690 non-null   int64  \n",
      " 4   Months_Inactive_12_mon         5690 non-null   int64  \n",
      " 5   Contacts_Count_12_mon          5690 non-null   int64  \n",
      " 6   Credit_Limit                   5690 non-null   float64\n",
      " 7   Total_Revolving_Bal            5690 non-null   int64  \n",
      " 8   Avg_Open_To_Buy                5690 non-null   float64\n",
      " 9   Total_Amt_Chng_Q4_Q1           5690 non-null   float64\n",
      " 10  Total_Trans_Amt                5690 non-null   int64  \n",
      " 11  Total_Trans_Ct                 5690 non-null   int64  \n",
      " 12  Total_Ct_Chng_Q4_Q1            5690 non-null   float64\n",
      " 13  Avg_Utilization_Ratio          5690 non-null   float64\n",
      " 14  Attrition_Flag                 5690 non-null   int64  \n",
      " 15  Gender_M                       5690 non-null   int64  \n",
      " 16  Education_Level_College        5690 non-null   int64  \n",
      " 17  Education_Level_Doctorate      5690 non-null   int64  \n",
      " 18  Education_Level_High School    5690 non-null   int64  \n",
      " 19  Education_Level_Post-Graduate  5690 non-null   int64  \n",
      " 20  Education_Level_Uneducated     5690 non-null   int64  \n",
      " 21  Marital_Status_Divorced        5690 non-null   int64  \n",
      " 22  Marital_Status_Single          5690 non-null   int64  \n",
      " 23  Income_Category_$120K +        5690 non-null   int64  \n",
      " 24  Income_Category_$40K - $60K    5690 non-null   int64  \n",
      " 25  Income_Category_$60K - $80K    5690 non-null   int64  \n",
      " 26  Income_Category_$80K - $120K   5690 non-null   int64  \n",
      " 27  Card_Category_Gold             5690 non-null   int64  \n",
      " 28  Card_Category_Platinum         5690 non-null   int64  \n",
      " 29  Card_Category_Silver           5690 non-null   int64  \n",
      "dtypes: float64(5), int64(25)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "#Assess new dataframe of dropped features bank_dropped_df_dummy_dropped\n",
    "bank_dropped_df_dummy_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Age</th>\n",
       "      <th>Dependent_count</th>\n",
       "      <th>Months_on_book</th>\n",
       "      <th>Total_Relationship_Count</th>\n",
       "      <th>Months_Inactive_12_mon</th>\n",
       "      <th>Contacts_Count_12_mon</th>\n",
       "      <th>Credit_Limit</th>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <th>Avg_Open_To_Buy</th>\n",
       "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
       "      <th>...</th>\n",
       "      <th>Education_Level_Uneducated</th>\n",
       "      <th>Marital_Status_Divorced</th>\n",
       "      <th>Marital_Status_Single</th>\n",
       "      <th>Income_Category_$120K +</th>\n",
       "      <th>Income_Category_$40K - $60K</th>\n",
       "      <th>Income_Category_$60K - $80K</th>\n",
       "      <th>Income_Category_$80K - $120K</th>\n",
       "      <th>Card_Category_Gold</th>\n",
       "      <th>Card_Category_Platinum</th>\n",
       "      <th>Card_Category_Silver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3625.0</td>\n",
       "      <td>2517</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>1.158</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>1926</td>\n",
       "      <td>794.0</td>\n",
       "      <td>0.602</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1438.3</td>\n",
       "      <td>648</td>\n",
       "      <td>790.3</td>\n",
       "      <td>0.477</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2550.0</td>\n",
       "      <td>1623</td>\n",
       "      <td>927.0</td>\n",
       "      <td>0.650</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>0.677</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_Age  Dependent_count  Months_on_book  Total_Relationship_Count  \\\n",
       "3            34                2              36                         4   \n",
       "4            49                2              39                         5   \n",
       "5            60                0              45                         5   \n",
       "8            30                0              36                         3   \n",
       "9            33                3              36                         5   \n",
       "\n",
       "   Months_Inactive_12_mon  Contacts_Count_12_mon  Credit_Limit  \\\n",
       "3                       3                      4        3625.0   \n",
       "4                       3                      4        2720.0   \n",
       "5                       2                      4        1438.3   \n",
       "8                       3                      2        2550.0   \n",
       "9                       2                      3        1457.0   \n",
       "\n",
       "   Total_Revolving_Bal  Avg_Open_To_Buy  Total_Amt_Chng_Q4_Q1  ...  \\\n",
       "3                 2517           1108.0                 1.158  ...   \n",
       "4                 1926            794.0                 0.602  ...   \n",
       "5                  648            790.3                 0.477  ...   \n",
       "8                 1623            927.0                 0.650  ...   \n",
       "9                    0           1457.0                 0.677  ...   \n",
       "\n",
       "   Education_Level_Uneducated  Marital_Status_Divorced  Marital_Status_Single  \\\n",
       "3                           0                        0                      1   \n",
       "4                           0                        0                      0   \n",
       "5                           0                        0                      0   \n",
       "8                           0                        0                      0   \n",
       "9                           0                        0                      1   \n",
       "\n",
       "   Income_Category_$120K +  Income_Category_$40K - $60K  \\\n",
       "3                        0                            0   \n",
       "4                        0                            1   \n",
       "5                        0                            0   \n",
       "8                        0                            0   \n",
       "9                        0                            0   \n",
       "\n",
       "   Income_Category_$60K - $80K  Income_Category_$80K - $120K  \\\n",
       "3                            0                             0   \n",
       "4                            0                             0   \n",
       "5                            0                             0   \n",
       "8                            0                             0   \n",
       "9                            0                             0   \n",
       "\n",
       "   Card_Category_Gold  Card_Category_Platinum  Card_Category_Silver  \n",
       "3                   0                       0                     0  \n",
       "4                   0                       0                     0  \n",
       "5                   0                       0                     0  \n",
       "8                   0                       0                     0  \n",
       "9                   0                       0                     0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_dropped_df_dummy_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5690, 30)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_dropped_df_dummy_dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Create Train_Test Split & Preprocess Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Create Train_Test_Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementation of future modeling, a train_test_split is created for both the missing and dropped datasets. The test size is arbritraily set for a 70/30 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating train_test_split for both missing and dropped bank churn datasets.\n",
    "#Test size arbritarily set at 70/30 split for training and future model performance and testing.\n",
    "\n",
    "X_missing = bank_missing_df_dummy_dropped.drop(['Attrition_Flag'], axis=1)\n",
    "y_missing = bank_missing_df_dummy_dropped['Attrition_Flag']\n",
    "X_dropped = bank_dropped_df_dummy_dropped.drop(['Attrition_Flag'], axis=1)\n",
    "y_dropped = bank_dropped_df_dummy_dropped['Attrition_Flag']\n",
    "\n",
    "\n",
    "X_train_missing, X_test_missing, y_train_missing, y_test_missing = train_test_split(X_missing, y_missing, test_size=0.30, random_state = random_number) \n",
    "X_train_dropped, X_test_dropped, y_train_dropped, y_test_dropped = train_test_split(X_dropped, y_dropped, test_size=0.30, random_state= random_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Apply StandardScaler() for Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create equal distributions with mean centered at 0 between each of the features for future modeling, a standard scaler object is created and defined for training for both datasets. The same scaler is also applied for transforming X_test datasets for consistency and future modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created function for fitting and transforming scaler object for X_train of both datasets and then transform X_test for both datasets\n",
    "def scaling(train , test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train)\n",
    "    X_test_scaled = scaler.transform(test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_missing , X_test_scaled_missing = scaling(X_train_missing, X_test_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_dropped , X_test_scaled_dropped = scaling(X_train_dropped, X_test_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing as well the above arrays created after scaling the trained datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41349078,  1.28797953, -1.35293228, ..., -0.11017496,\n",
       "        -0.04408862, -0.24090084],\n",
       "       [ 1.45572316, -1.03949649,  0.0039595 , ..., -0.11017496,\n",
       "        -0.04408862, -0.24090084],\n",
       "       [ 0.33419479,  1.28797953, -0.85951709, ..., -0.11017496,\n",
       "        -0.04408862, -0.24090084],\n",
       "       ...,\n",
       "       [ 0.08496627,  2.06380487,  0.6207285 , ..., -0.11017496,\n",
       "        -0.04408862, -0.24090084],\n",
       "       [ 0.08496627,  0.51215419,  0.8674361 , ..., -0.11017496,\n",
       "        -0.04408862, -0.24090084],\n",
       "       [ 1.58033743, -1.03949649,  2.10097409, ..., -0.11017496,\n",
       "        -0.04408862, -0.24090084]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reviewing of X_train_scaled_missing and dropped as well as X_test_scaled_missing and dropped\n",
    "X_train_scaled_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.33110890e+00, -2.63671151e-01,  3.95950466e-03, ...,\n",
       "        -1.10174961e-01, -4.40886190e-02, -2.40900841e-01],\n",
       "       [-4.13490784e-01,  1.28797953e+00, -2.42748093e-01, ...,\n",
       "        -1.10174961e-01, -4.40886190e-02, -2.40900841e-01],\n",
       "       [-1.90886194e+00, -1.03949649e+00, -1.72299368e+00, ...,\n",
       "        -1.10174961e-01, -4.40886190e-02, -2.40900841e-01],\n",
       "       ...,\n",
       "       [-2.88876521e-01, -1.03949649e+00, -2.42748093e-01, ...,\n",
       "        -1.10174961e-01, -4.40886190e-02,  4.15108555e+00],\n",
       "       [ 2.09580531e-01,  5.12154188e-01, -7.36163289e-01, ...,\n",
       "        -1.10174961e-01, -4.40886190e-02, -2.40900841e-01],\n",
       "       [ 3.34194794e-01,  1.28797953e+00,  8.67436097e-01, ...,\n",
       "        -1.10174961e-01, -4.40886190e-02, -2.40900841e-01]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.50889427,  0.52736863, -0.98537568, ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724],\n",
       "       [-0.63893519, -1.02914063, -0.48336093, ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724],\n",
       "       [ 0.35530376,  2.08387789, -0.10684986, ...,  9.5722467 ,\n",
       "        -0.03884166, -0.23532724],\n",
       "       ...,\n",
       "       [ 1.47382257,  1.30562326,  2.15221653, ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724],\n",
       "       [ 1.22526283, -1.02914063,  1.02268333, ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724],\n",
       "       [-1.50889427, -0.250886  , -2.86793101, ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.13605466,  1.30562326,  0.01865382, ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724],\n",
       "       [ 0.35530376,  1.30562326,  0.01865382, ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724],\n",
       "       [ 0.85242323,  0.52736863,  1.3991944 , ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724],\n",
       "       ...,\n",
       "       [-0.88749493, -0.250886  , -0.60886462, ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724],\n",
       "       [-0.26609559,  1.30562326, -0.73436831, ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724],\n",
       "       [-1.26033453,  0.52736863,  0.01865382, ..., -0.10446868,\n",
       "        -0.03884166, -0.23532724]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that arrays have been created for X_train, X_test, y_train, and y_test for each missing and dropped datasets, these will be utilized in the next steps regarding assessing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled_missing shape: (5670, 32) and y_train_missing shape: (5670,)\n"
     ]
    }
   ],
   "source": [
    "#Missing X_train and y_train set shape\n",
    "print(f\"X_train_scaled_missing shape: {X_train_scaled_missing.shape} and y_train_missing shape: {y_train_missing.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_scaled_missing shape: (2431, 32) and y_test_missing shape: (2431,)\n"
     ]
    }
   ],
   "source": [
    "#Missing X_test and y_test set shape\n",
    "print(f\"X_test_scaled_missing shape: {X_test_scaled_missing.shape} and y_test_missing shape: {y_test_missing.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled_dropped shape: (3983, 29) and y_train_dropped shape: (3983,)\n"
     ]
    }
   ],
   "source": [
    "#Dropped X_train and y_train set shape\n",
    "print(f\"X_train_scaled_dropped shape: {X_train_scaled_dropped.shape} and y_train_dropped shape: {y_train_dropped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_scaled_dropped shape: (1707, 29) and y_test_dropped shape: (1707,)\n"
     ]
    }
   ],
   "source": [
    "#Dropped X_test and y_test set shape\n",
    "print(f\"X_test_scaled_dropped shape: {X_test_scaled_dropped.shape} and y_test_dropped shape: {y_test_dropped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created dataframes for the new and respectice X_train_scaled and y_test arrays\n",
    "#Code for setting arrays as df and then saving to .csv found on https://stackoverflow.com/questions/6081008/dump-a-numpy-array-into-a-csv-file\n",
    "df_X_train_scaled_missing = pd.DataFrame(X_train_scaled_missing)\n",
    "df_y_train_missing = pd.DataFrame(y_train_missing)\n",
    "df_X_test_scaled_missing = pd.DataFrame(X_test_scaled_missing)\n",
    "df_y_test_missing = pd.DataFrame(y_test_missing)\n",
    "df_X_train_scaled_dropped = pd.DataFrame(X_train_scaled_dropped)\n",
    "df_y_train_dropped = pd.DataFrame(y_train_dropped)\n",
    "df_X_test_scaled_dropped = pd.DataFrame(X_test_scaled_dropped)\n",
    "df_y_test_dropped = pd.DataFrame(y_test_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new datasets to new csv files for future model implementation\n",
    "datapath = 'C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets'\n",
    "df_X_train_scaled_missing.to_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/X_train_scaled_missing.csv', header=True, index=True) \n",
    "df_y_train_missing.to_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/y_train_missing.csv', header=True, index=False) \n",
    "df_X_test_scaled_missing.to_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/X_test_scaled_missing.csv', header=True, index=False)\n",
    "df_y_test_missing.to_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/y_test_missing.csv', header=True, index=False)\n",
    "df_X_train_scaled_dropped.to_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/X_train_scaled_dropped.csv', header=True, index=False)\n",
    "df_y_train_dropped.to_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/y_train_dropped.csv', header=True, index=False)\n",
    "df_X_test_scaled_dropped.to_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/X_test_scaled_dropped.csv', header=True, index=False)\n",
    "df_y_test_dropped.to_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/y_test_dropped.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying scikit learns train_test_split and StandardScaler in order to create arbitray 70/30 splits of the datasets for both the missing and dropped bank churn datasets, these will be applied in the next steps regarding model performance. Some notes regarding the steps involved include avoiding the dummy variable trap or multicollineratiy between one-hot encoded feature variables by instead apply only k-1 feature values for the categorical variables. And a function was also created in order to apply the StandardScaler() object for the two different datasets as otherwise the object could not fit_transform for a different dataset after already being applied to a previous one. \n",
    "\n",
    "Notes to keep in mind is especially for the dropped dataset, the training data because of starting being dropped for missing values has further decreased for a training set of 3983 observations. This further decrease in terms of observations might affect different model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
